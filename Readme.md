### ML Training Techniques
This project summarizes different building blocks of ML training techniques to stabilize and speed up training of deep neural networks. The focus is on the training process itself, not on the model architecture. The goal is to provide a comprehensive overview of the most important techniques and to give a guideline for the practical use of these techniques.
Not considered are model changes, like changing the model architecture or the type of loss function.
The techniques are shown using the MNIST dataset and a simple CNN model. The code is written in Pytorch.
Planned is:
- Learning Rate Scheduling
- Random Weight Initialization in Pytorch
- Weighted Data Sampling
- Data Augmentation
- Layerwise Learning Rate & Weight Decay
- Supervised Pretraining (Transfer Learning)
- Self-Supervised Pretraining Masked Image Modeling
- Self-Supervised Contrastive Learning like MoCo
- Distillation for Model Regularization comparison of intermediate layer outputs
- Semi-Supervised Learning


